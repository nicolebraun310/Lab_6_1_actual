{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "35ba4434-6f56-4008-b388-dbb9a1d2be6a",
      "metadata": {
        "id": "35ba4434-6f56-4008-b388-dbb9a1d2be6a"
      },
      "outputs": [],
      "source": [
        "import polars as pl"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60887d35-6168-4b46-b1d8-077856f351f4",
      "metadata": {
        "id": "60887d35-6168-4b46-b1d8-077856f351f4"
      },
      "source": [
        "# Why use relative addresses?\n",
        "\n",
        "In this notebook, we will illustrate\n",
        "\n",
        "1. That relative addresses for loading data files works, but\n",
        "2. Using absolute addresses for loading data files *will not*."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e71dd7c-a5df-49da-af06-279adb78ab7a",
      "metadata": {
        "id": "9e71dd7c-a5df-49da-af06-279adb78ab7a"
      },
      "source": [
        "## Problem 1 - Load the `lat_long_example.csv` file using a relative address.\n",
        "\n",
        "**Tasks.**\n",
        "1. Open a terminal, start `nu`, and navigate to the root menu of your first/primary data repository,\n",
        "2. Use `ls **/*` to get the relative address of `lat_long_example.csv`, and\n",
        "3. Use `polars` to load and inspect these data using this relative path."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "449783a9-db7e-4d92-b45a-2ba0088916bd",
      "metadata": {
        "id": "449783a9-db7e-4d92-b45a-2ba0088916bd"
      },
      "outputs": [],
      "source": [
        "relative_path = \"./data/lat_long_examples.csv\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "metadata": {
        "id": "Uwv5kITt-MQY"
      },
      "id": "Uwv5kITt-MQY",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://raw.githubusercontent.com/nicolebraun310/Lab_6_1_actual/refs/heads/main/data/lat_long_examples.csv -o ./sample_data/lat_long_examples.csv"
      ],
      "metadata": {
        "id": "d303o0yb_KHh",
        "outputId": "d69a1ac0-09fb-4964-c5c1-d2a59610ac9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "d303o0yb_KHh",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100   206  100   206    0     0   1966      0 --:--:-- --:--:-- --:--:--  1980\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "79d2bdb3-4125-4027-9110-387b75cbfe6d",
      "metadata": {
        "id": "79d2bdb3-4125-4027-9110-387b75cbfe6d",
        "outputId": "752d99cd-07f0-4a90-c0eb-c40a7d1036b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[City 1: string, Lat 1: string, Long 1: string, City 2: string, Lat 2: string, Long 2: string, Distance from Web (km): string]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "(lat_long_examples :=\n",
        " spark.read.csv('./sample_data/lat_long_examples.csv', header=True)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a173049f-b638-426f-adb3-0a9a13251ccb",
      "metadata": {
        "id": "a173049f-b638-426f-adb3-0a9a13251ccb"
      },
      "source": [
        "## Problem 2 - Load the `lat_long_example.csv` file using a absolute address.\n",
        "\n",
        "**Tasks.**\n",
        "1. Open a terminal, start `nu`, and navigate to the root menu of *one of your first/primary data repository,\n",
        "2. Use `glob **/*` to get the absolute address of `lat_long_example.csv`, and\n",
        "3. Use `polars` to load and inspect these data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "68846b35-bf00-43ee-8de9-893a70a2c389",
      "metadata": {
        "id": "68846b35-bf00-43ee-8de9-893a70a2c389"
      },
      "outputs": [],
      "source": [
        "absolute_path = \"C: /Users/fx3734qp/OneDrive - Minnesota State/health_survey/data/lat_long_examples.csv\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f658194-ada1-4a12-bca0-2dfda1e00eeb",
      "metadata": {
        "id": "3f658194-ada1-4a12-bca0-2dfda1e00eeb"
      },
      "source": [
        "## Illustrating the problem with absolute addresses\n",
        "\n",
        "While the relative address in problem 1 points to the data IN THIS COPY of the repo, the absolute address points to the data in EXACTLY one of the copies of the repository. This becomes a problem if (A) anything changes in that repository, or (B) we are working on a different machine.\n",
        "\n",
        "**Tasks.** To illustrate why this is a problem, do the following.\n",
        "\n",
        "1. From your first/primary repository commit and push this notebook to GitHub,\n",
        "2. Fetch and pull this notebook to another local copy of the repository,\n",
        "3. In your file explorer (Files or Finder), move your first/primary repository into another folder, e.g., make a new folder and drag-and-drop the repo.\n",
        "4. Rerun the cells in each local copy of the repository and document your findings in the WORD document."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://raw.githubusercontent.com/nicolebraun310/Lab_6_1_actual/refs/heads/main/data/health_survey.csv -o ./sample_data/health_survey.csv"
      ],
      "metadata": {
        "id": "I_YWvBEz_xpS",
        "outputId": "9d3f82f4-0b0e-41cd-f292-fa9ec3ee9350",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "I_YWvBEz_xpS",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  219k  100  219k    0     0   968k      0 --:--:-- --:--:-- --:--:--  969k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(health_survey :=\n",
        " spark.read.csv('./sample_data/health_survey.csv', header=True)\n",
        ")"
      ],
      "metadata": {
        "id": "npqGv4zgABTc",
        "outputId": "5ec4fe9e-dde1-4677-8cf1-0ca859bb066b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "npqGv4zgABTc",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[_c0: string, F1: string, F5: string, F2: string, F1.1: string, F2.1: string, F6: string, F4: string, F3: string, F5.1: string, F1.2: string, F2.2: string, F6.1: string, F2.3: string, F4.1: string, F2.4: string, F5.2: string, F2.5: string, F6.2: string, F1.3: string, F2.6: string, F5.3: string, F4.2: string, F2.7: string, F3.1: string, F2.8: string, F5.4: string, F3.2: string, F1.4: string, F3.3: string, F1.5: string, F5.5: string, F6.3: string, F1.6: string, F5.6: string, F2.9: string, F3.4: string, F4.3: string, F2.10: string, F1.7: string, F6.4: string, F4.4: string, F5.7: string, F3.5: string, F2.11: string]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://raw.githubusercontent.com/nicolebraun310/Lab_6_1_actual/refs/heads/main/data/ReverseCodingItems.csv -o ./sample_data/ReverseCodingItems.csv"
      ],
      "metadata": {
        "id": "tSB0Nje-AHb7",
        "outputId": "037b445a-c3a2-45db-aaa7-01937385b754",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "tSB0Nje-AHb7",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  3162  100  3162    0     0  21329      0 --:--:-- --:--:-- --:--:-- 21364\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(ReverseCodingItems :=\n",
        " spark.read.csv('./sample_data/ReverseCodingItems.csv', header=True)\n",
        ")"
      ],
      "metadata": {
        "id": "q6LX3u0AAPeq",
        "outputId": "d7188d59-a6d5-4ce1-ff19-0c352cf07789",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "q6LX3u0AAPeq",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[Question: string, Construct: string, Question # on Qualtrics Survey: string, Needs Reverse Coding?: string, Column Name: string]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, explode, array, lit\n",
        "survey_no_id = health_survey.drop('ID')\n",
        "\n",
        "melted = survey_no_id.selectExpr(\"stack({0}, {1}) as (variable, value)\".format(\n",
        "    len(survey_no_id.columns),\n",
        "    \", \".join([f\"'{c}', `{c}`\" for c in survey_no_id.columns])\n",
        "))\n",
        "unique_values = melted.select(\"value\").distinct()\n",
        "value_list = [row[\"value\"] for row in unique_values.collect()]\n"
      ],
      "metadata": {
        "id": "oNDsjHqpAth3"
      },
      "id": "oNDsjHqpAth3",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reg = {\n",
        "    'Strongly Agree': 5,\n",
        "    'Somewhat Agree': 4,\n",
        "    'Strongly Disagree': 3,\n",
        "    'Neither Agree nor Disagree': 2,\n",
        "    'Somewhat Disagree': 1\n",
        "}"
      ],
      "metadata": {
        "id": "7N_L3OcyA9hQ"
      },
      "id": "7N_L3OcyA9hQ",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rev = {'Strongly Agree':1,\n",
        "         'Somewhat Agree':2,\n",
        "         'Strongly Disagree':3,\n",
        "         'Neither Agree nor Disagree':4,\n",
        "         'Somewhat Disagree':5,\n",
        "        }"
      ],
      "metadata": {
        "id": "fH-y2x57BAJV"
      },
      "id": "fH-y2x57BAJV",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q_col = list(filter(lambda c: c != 'ID', health_survey.columns))"
      ],
      "metadata": {
        "id": "yzZqHvcpBPId"
      },
      "id": "yzZqHvcpBPId",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "health_survey_summary_df = health_survey.selectExpr(\n",
        "    \"ID\",\n",
        "    \"stack({0}, {1}) as (Question, Response)\".format(\n",
        "        len(q_col),\n",
        "        \", \".join([f\"'{col_name}', `{col_name}`\" for col_name in q_col])\n",
        "    )\n",
        ")\n"
      ],
      "metadata": {
        "id": "Z33Rt_CTB48z",
        "outputId": "be67edb5-67ae-47c3-9472-e305d41d2769",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        }
      },
      "id": "Z33Rt_CTB48z",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `ID` cannot be resolved. Did you mean one of the following? [`F1`, `F2`, `F3`, `F4`, `F5`].; line 1 pos 0;\n'Project ['ID, Question#263, Response#264]\n+- Generate stack(45, _c0, _c0#48, F1, F1#49, F5, F5#50, F2, F2#51, F1.1, F1.1#52, F2.1, F2.1#53, F6, F6#54, F4, F4#55, F3, F3#56, F5.1, F5.1#57, F1.2, F1.2#58, F2.2, ... 67 more fields), false, [Question#263, Response#264]\n   +- Relation [_c0#48,F1#49,F5#50,F2#51,F1.1#52,F2.1#53,F6#54,F4#55,F3#56,F5.1#57,F1.2#58,F2.2#59,F6.1#60,F2.3#61,F4.1#62,F2.4#63,F5.2#64,F2.5#65,F6.2#66,F1.3#67,F2.6#68,F5.3#69,F4.2#70,F2.7#71,... 21 more fields] csv\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-998677269.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m health_survey_summary_df = health_survey.selectExpr(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"ID\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \"stack({0}, {1}) as (Question, Response)\".format(\n\u001b[1;32m      4\u001b[0m         \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_col\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;34m\", \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf\"'{col_name}', `{col_name}`\"\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mq_col\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mselectExpr\u001b[0;34m(self, *expr)\u001b[0m\n\u001b[1;32m   3265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3266\u001b[0m             \u001b[0mexpr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# type: ignore[assignment]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3267\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselectExpr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jseq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3268\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `ID` cannot be resolved. Did you mean one of the following? [`F1`, `F2`, `F3`, `F4`, `F5`].; line 1 pos 0;\n'Project ['ID, Question#263, Response#264]\n+- Generate stack(45, _c0, _c0#48, F1, F1#49, F5, F5#50, F2, F2#51, F1.1, F1.1#52, F2.1, F2.1#53, F6, F6#54, F4, F4#55, F3, F3#56, F5.1, F5.1#57, F1.2, F1.2#58, F2.2, ... 67 more fields), false, [Question#263, Response#264]\n   +- Relation [_c0#48,F1#49,F5#50,F2#51,F1.1#52,F2.1#53,F6#54,F4#55,F3#56,F5.1#57,F1.2#58,F2.2#59,F6.1#60,F2.3#61,F4.1#62,F2.4#63,F5.2#64,F2.5#65,F6.2#66,F1.3#67,F2.6#68,F5.3#69,F4.2#70,F2.7#71,... 21 more fields] csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, when, split, expr\n",
        "melted = health_survey.selectExpr(\"ID\", \"stack({0}, {1}) as (Question, Response)\".format(\n",
        "    len(q_col),\n",
        "    \", \".join([f\"'{c}', `{c}`\" for c in q_col])\n",
        "))\n",
        "\n",
        "melted = (melted\n",
        "    .replace(reg, subset=[\"Response\"])\n",
        "    .withColumn(\"reg_coding\", col(\"Response\").cast(\"int\"))\n",
        "    .replace(rev, subset=[\"Response\"])\n",
        "    .withColumn(\"rev_coding\", col(\"Response\").cast(\"int\"))\n",
        ")\n",
        "\n",
        "joined = melted.join(ReverseCodingItems, melted[\"Question\"] == ReverseCodingItems[\"Column Name\"], how=\"left\")\n",
        "\n",
        "joined = joined.withColumn(\n",
        "    \"coding\",\n",
        "    when(col(\"Needs Reverse Coding?\") == \"Yes\", col(\"rev_coding\")).otherwise(col(\"reg_coding\"))\n",
        ").withColumn(\n",
        "    \"q_type\",\n",
        "    split(col(\"Question\"), \"\\\\.\").getItem(0)\n",
        ")\n",
        "\n",
        "health_survey_summary_df = joined.groupBy(\"ID\").pivot(\"q_type\").sum(\"coding\")"
      ],
      "metadata": {
        "id": "TTGxVOdbBfQp",
        "outputId": "154fe032-f80a-4561-d4cd-ddcfd043f6b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        }
      },
      "id": "TTGxVOdbBfQp",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `ID` cannot be resolved. Did you mean one of the following? [`F1`, `F2`, `F3`, `F4`, `F5`].; line 1 pos 0;\n'Project ['ID, Question#261, Response#262]\n+- Generate stack(45, _c0, _c0#48, F1, F1#49, F5, F5#50, F2, F2#51, F1.1, F1.1#52, F2.1, F2.1#53, F6, F6#54, F4, F4#55, F3, F3#56, F5.1, F5.1#57, F1.2, F1.2#58, F2.2, ... 67 more fields), false, [Question#261, Response#262]\n   +- Relation [_c0#48,F1#49,F5#50,F2#51,F1.1#52,F2.1#53,F6#54,F4#55,F3#56,F5.1#57,F1.2#58,F2.2#59,F6.1#60,F2.3#61,F4.1#62,F2.4#63,F5.2#64,F2.5#65,F6.2#66,F1.3#67,F2.6#68,F5.3#69,F4.2#70,F2.7#71,... 21 more fields] csv\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4206628867.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m melted = health_survey.selectExpr(\"ID\", \"stack({0}, {1}) as (Question, Response)\".format(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_col\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m\", \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf\"'{c}', `{c}`\"\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mq_col\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m ))\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mselectExpr\u001b[0;34m(self, *expr)\u001b[0m\n\u001b[1;32m   3265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3266\u001b[0m             \u001b[0mexpr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# type: ignore[assignment]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3267\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselectExpr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jseq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3268\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `ID` cannot be resolved. Did you mean one of the following? [`F1`, `F2`, `F3`, `F4`, `F5`].; line 1 pos 0;\n'Project ['ID, Question#261, Response#262]\n+- Generate stack(45, _c0, _c0#48, F1, F1#49, F5, F5#50, F2, F2#51, F1.1, F1.1#52, F2.1, F2.1#53, F6, F6#54, F4, F4#55, F3, F3#56, F5.1, F5.1#57, F1.2, F1.2#58, F2.2, ... 67 more fields), false, [Question#261, Response#262]\n   +- Relation [_c0#48,F1#49,F5#50,F2#51,F1.1#52,F2.1#53,F6#54,F4#55,F3#56,F5.1#57,F1.2#58,F2.2#59,F6.1#60,F2.3#61,F4.1#62,F2.4#63,F5.2#64,F2.5#65,F6.2#66,F1.3#67,F2.6#68,F5.3#69,F4.2#70,F2.7#71,... 21 more fields] csv\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}